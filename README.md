# ğŸ¦™ LLaMA-2 7B GPTQ RAG (Retrieval-Augmented Generation) Pipeline

## ğŸ‡¬ğŸ‡§ English Description

This project implements a RAG (Retrieval-Augmented Generation) system using:

- ğŸ¤– **LLaMA-2 7B GPTQ** from TheBloke (quantized for fast inference)
- ğŸ”— **LangChain** for orchestration
- ğŸ§  **HuggingFace Transformers** for model and tokenizer
- ğŸ§  **Multilingual-E5 Embeddings** for document representation
- ğŸ“¦ **Chroma DB** for vector storage
- ğŸ’¡ **RetrievalQA chain** for answering questions based on context

### ğŸ” What it does

This pipeline allows users to ask questions, and the system will:
1. Retrieve the most relevant context from a document database.
2. Generate an answer using LLaMA-2 based only on that context.

If the answer isn't found in the context, the model will say:
> "I don't know."

### âœ… Features
- GPU-accelerated with CUDA
- GPTQ support for memory-efficient inference
- Streamed generation output (optional)
- Easy to integrate into apps using LangChain

---

## ğŸ‡¹ğŸ‡­ à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸ à¸²à¸©à¸²à¹„à¸—à¸¢

à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œà¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸£à¸°à¸šà¸š RAG (Retrieval-Augmented Generation) à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µ:

- ğŸ¤– **LLaMA-2 7B GPTQ** à¸ˆà¸²à¸ TheBloke (à¹€à¸§à¸­à¸£à¹Œà¸Šà¸±à¸™à¸—à¸µà¹ˆà¸–à¸¹à¸ quantize à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸£à¸±à¸™à¹„à¸”à¹‰à¹€à¸£à¹‡à¸§à¹à¸¥à¸°à¹€à¸šà¸²)
- ğŸ”— **LangChain** à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸±à¸”à¸à¸²à¸£ pipeline
- ğŸ§  **HuggingFace Transformers** à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸° tokenizer
- ğŸ§  **Multilingual-E5 Embeddings** à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¹€à¸§à¸à¹€à¸•à¸­à¸£à¹Œ
- ğŸ“¦ **Chroma DB** à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¹€à¸§à¸à¹€à¸•à¸­à¸£à¹Œ
- ğŸ’¡ **RetrievalQA chain** à¸ªà¸³à¸«à¸£à¸±à¸šà¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸ˆà¸²à¸ context à¸—à¸µà¹ˆà¸”à¸¶à¸‡à¸¡à¸²à¹„à¸”à¹‰

### ğŸ” à¸£à¸°à¸šà¸šà¸™à¸µà¹‰à¸—à¸³à¸­à¸°à¹„à¸£

à¹€à¸¡à¸·à¹ˆà¸­à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸•à¸±à¹‰à¸‡à¸„à¸³à¸–à¸²à¸¡ à¸£à¸°à¸šà¸šà¸ˆà¸°:
1. à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸£à¸·à¸­ context à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸ˆà¸²à¸à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸§à¸à¹€à¸•à¸­à¸£à¹Œ
2. à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¸ˆà¸²à¸ context à¸™à¸±à¹‰à¸™à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ LLaMA-2

à¸«à¸²à¸à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸«à¸²à¸„à¸³à¸•à¸­à¸šà¸ˆà¸²à¸ context à¹„à¸”à¹‰ à¸£à¸°à¸šà¸šà¸ˆà¸°à¸•à¸­à¸šà¸§à¹ˆà¸²:
> "I don't know."

### âœ… à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´à¹€à¸”à¹ˆà¸™
- à¸£à¸­à¸‡à¸£à¸±à¸š CUDA à¸šà¸™ GPU
- à¹ƒà¸Šà¹‰ GPTQ à¸›à¸£à¸°à¸«à¸¢à¸±à¸”à¹à¸£à¸¡à¹à¸¥à¸°à¹€à¸£à¹‡à¸§
- à¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¹à¸ªà¸”à¸‡à¸œà¸¥à¹à¸šà¸š stream (option)
- à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸‡à¹ˆà¸²à¸¢à¸œà¹ˆà¸²à¸™ LangChain API

---

## ğŸš€ Quick Start

```bash
# Clone the project
git clone <your-repo-url>
cd <your-project-folder>

# Create environment and install dependencies
conda create -n rag_llama2 python=3.10
conda activate rag_llama2
pip install -r requirements.txt

# (Optional) Install gptqmodel if using GPTQ
pip install --prefer-binary gptqmodel

# Run the app
python app_gradio.py

Folder Structure
â”œâ”€â”€ app_gradio.py         # Main Gradio app
â”œâ”€â”€ rag_pipeline.py       # LangChain pipeline setup
â”œâ”€â”€ db/                   # Chroma vector database
â”œâ”€â”€ requirements.txt      # All Python dependencies
â””â”€â”€ README.md             # This file
